{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lazarmiovcic/nanoBertic/blob/master/nanoBerti%C4%87.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MYDEOITVaGGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4656e45a-2732-4def-e5ab-04131de7cc5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check tokenizer vocab file\n",
        "print(os.path.exists('/content/drive/Othercomputers/My_Laptop/all_text_batches'))\n",
        "\n",
        "# List training batch files (shows first few)\n",
        "train_dir = '/content/drive/Othercomputers/My_Laptop/all_text_batches/train'\n",
        "if os.path.exists(train_dir):\n",
        "    print(os.listdir(train_dir)[:5]) # Print first 5 files\n",
        "else:\n",
        "    print(f\"Directory not found: {train_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoknFi4V-trX",
        "outputId": "8e7056ac-e726-46d7-cfc2-6775d5bbc375"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "['train_batch_1229.txt', 'train_batch_1230.txt', 'train_batch_1231.txt', 'train_batch_1234.txt', 'train_batch_1242.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "import math\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "jdZ2MpIBFgO0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 64\n",
        "max_len = 128\n",
        "batch_size = 64\n",
        "n_layers = 2\n",
        "n_heads = 2\n",
        "dropout = 0.1\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "o8rZ6z7dh6BR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertDataset_v1(Dataset):\n",
        "    def __init__(self, path_to_data_dir, tokenizer, seq_len,):\n",
        "        self.paths = [str(x) for x in Path(path_to_data_dir).glob('**/*.txt')] # Use path_to_data_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_len = seq_len\n",
        "        self.pad_token_id = self.tokenizer.vocab['[PAD]'] # Store PAD ID for convenience\n",
        "\n",
        "        self.total_samples = 0\n",
        "        self.file_line_counts = [] # Stores (file_path, num_lines_in_file)\n",
        "        self.cumulative_line_offsets = [0] # Stores cumulative sums of lines for quick lookup\n",
        "\n",
        "        print(\"\\n[Dataset Init] Counting lines across all batch files...\")\n",
        "        for p in self.paths:\n",
        "            with open(p, 'r', encoding='utf-8') as f:\n",
        "                num_lines_in_file = sum(1 for _ in f) # Efficiently count lines\n",
        "            self.file_line_counts.append((p, num_lines_in_file))\n",
        "            self.total_samples += num_lines_in_file\n",
        "            self.cumulative_line_offsets.append(self.total_samples) # Add cumulative sum\n",
        "\n",
        "        if self.total_samples == 0:\n",
        "            raise ValueError(f\"No lines found in any files in {path_to_data_dir}. Check your data.\")\n",
        "        print(f\"[Dataset Init] Found {self.total_samples} total samples across {len(self.paths)} files.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_samples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Handle index out of bounds (shouldn't happen if __len__ is correct, but good for robustness)\n",
        "        if not (0 <= index < self.total_samples):\n",
        "            raise IndexError(f\"Index {index} is out of bounds for dataset of size {self.total_samples}\")\n",
        "\n",
        "        # Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\n",
        "        t1_str, t2_str, is_next_label = self.get_sent(index)\n",
        "\n",
        "        # Tokenize and get input IDs for t1 and t2 (after stripping)\n",
        "        # Note: self.tokenizer('string')['input_ids'][1:-1]\n",
        "        # This will remove [CLS] and [SEP] added by the tokenizer for single sentences.\n",
        "        t1_token_ids = self.tokenizer(t1_str)['input_ids'][1:-1]\n",
        "        t2_token_ids = self.tokenizer(t2_str)['input_ids'][1:-1]\n",
        "\n",
        "        # Step 2: replace random words in sentence with mask / random words\n",
        "        t1_random, t1_label = self.random_word(t1_token_ids)  # Pass token_ids, not string\n",
        "        t2_random, t2_label = self.random_word(t2_token_ids)  # Pass token_ids, not string\n",
        "\n",
        "        # Step 3: Adding CLS and SEP tokens to the start and end of sentences\n",
        "        # Adding PAD token for labels\n",
        "        cls_id = self.tokenizer.vocab['[CLS]']\n",
        "        sep_id = self.tokenizer.vocab['[SEP]']\n",
        "        pad_id = self.tokenizer.vocab['[PAD]']  # Use stored pad_id\n",
        "\n",
        "        t1_final = [cls_id] + t1_random + [sep_id]\n",
        "        t2_final = t2_random + [sep_id]  # T2 does not start with CLS in BERT\n",
        "\n",
        "        # Labels for MLM must correspond to the final input sequence.\n",
        "        # Pad t1_label and t2_label with PAD_ID for tokens that are not masked.\n",
        "        t1_label_final = [pad_id] + t1_label + [pad_id]\n",
        "        t2_label_final = t2_label + [pad_id]\n",
        "\n",
        "        # Step 4: combine sentence 1 and 2 as one input\n",
        "        # adding PAD tokens to make the sentence same length as seq_len\n",
        "        # The segment label for t1 is 0, for t2 is 1.\n",
        "        segment_label = [1 for _ in range(len(t1_final))] + [2 for _ in range(len(t2_final))]\n",
        "\n",
        "        bert_input = (t1_final + t2_final)[:self.seq_len]\n",
        "        bert_label = (t1_label_final + t2_label_final)[:self.seq_len]\n",
        "        segment_label = segment_label[:self.seq_len]  # Ensure segment_label is also truncated\n",
        "\n",
        "        # Calculate padding needed\n",
        "        padding_length = self.seq_len - len(bert_input)\n",
        "        if padding_length < 0:  # Should not happen with [:self.seq_len] but as a safeguard\n",
        "            padding_length = 0\n",
        "\n",
        "        padding_list = [pad_id for _ in range(padding_length)]\n",
        "\n",
        "        bert_input.extend(padding_list)\n",
        "        bert_label.extend(padding_list)\n",
        "        segment_label.extend(padding_list)\n",
        "\n",
        "        output = {\"input_ids\": bert_input,\n",
        "                  \"input_label\": bert_label,\n",
        "                  \"segment_label\": segment_label,\n",
        "                  \"is_next\": is_next_label}\n",
        "\n",
        "        return {key: torch.tensor(value) for key, value in output.items()}\n",
        "\n",
        "    def random_word(self, token_ids):  # Now expects list of token_ids\n",
        "        output = []\n",
        "        output_label = []\n",
        "        for tok_id in token_ids:\n",
        "            prob = random.random()\n",
        "            if prob < 0.15:\n",
        "                prob /= 0.15\n",
        "                if prob < 0.8:  # 80% chance change token to mask token\n",
        "                    output.append(self.tokenizer.vocab['[MASK]'])\n",
        "                elif prob < 0.9:  # 10% chance change token to random token\n",
        "                    output.append(random.randrange(len(self.tokenizer.vocab)))\n",
        "                else:  # 10% chance change token to current token\n",
        "                    output.append(tok_id)\n",
        "                output_label.append(tok_id)  # Original token ID is the label for these 15%\n",
        "            else:  # 85% chance: token left unchanged\n",
        "                output.append(tok_id)\n",
        "                output_label.append(self.pad_token_id)  # Label is PAD for unmasked tokens (don't predict)\n",
        "        return output, output_label\n",
        "\n",
        "    def get_file_lines(self, path):\n",
        "        with open(path, 'r', encoding=\"utf-8\") as f:\n",
        "            return f.readlines()\n",
        "\n",
        "    def get_sent(self, index):\n",
        "        # Find which file and line number within that file the index corresponds to\n",
        "        # Uses cumulative_line_offsets to efficiently find the file\n",
        "        file_idx = 0\n",
        "        for i in range(len(self.cumulative_line_offsets) - 1):\n",
        "            if self.cumulative_line_offsets[i] <= index < self.cumulative_line_offsets[i+1]:\n",
        "                file_idx = i\n",
        "                break\n",
        "        line_idx_in_file = index - self.cumulative_line_offsets[file_idx]\n",
        "\n",
        "        lines = self.get_file_lines(self.paths[file_idx])\n",
        "        # Get t1\n",
        "        t1_str = lines[line_idx_in_file]\n",
        "\n",
        "        prob = random.random()\n",
        "        if prob > 0.5: # Positive Pair\n",
        "            # Try to get the next line from the same file\n",
        "            if line_idx_in_file < self.file_line_counts[file_idx][1] - 1: # [1] is num_lines_in_file\n",
        "                t2_str = lines[line_idx_in_file+1]\n",
        "                return t1_str, t2_str, 1\n",
        "            else: # Last line of current file, try to get from next file\n",
        "                if file_idx < len(self.paths) - 1: # Check if there's a next file\n",
        "                    with open(self.paths[file_idx+1], 'r', encoding='utf-8') as next_file:\n",
        "                        t2_str = next_file.readline()\n",
        "                    return t1_str, t2_str, 1\n",
        "                else: # Last line of the entire corpus\n",
        "                    random_line_from_file = lines[random.randrange(len(lines))]\n",
        "                    return t1_str, random_line_from_file, 0 # Now it's a negative pair\n",
        "\n",
        "        else: # Negative Pair\n",
        "            random_line_from_file = lines[random.randrange(len(lines))]\n",
        "            return t1_str, random_line_from_file, 0"
      ],
      "metadata": {
        "id": "Clm5IGDyB0dE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=128):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.requires_grad = False\n",
        "\n",
        "        for pos in range(max_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((i + 1) / d_model)))\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self):\n",
        "        return self.pe\n",
        "\n",
        "\n",
        "class BERTEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size, seq_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.token = nn.Embedding(vocab_size, d_model, padding_idx=0)    # (seq_len, d_model)\n",
        "        self.segment = nn.Embedding(3, d_model, padding_idx=0)           # (seq_len, d_model)\n",
        "        self.position = PositionEmbedding(d_model, seq_len)              # (seq_len, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, ids, segment_label):\n",
        "        x = self.token(ids) + self.segment(segment_label) + self.position()\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.head_size = d_model // n_heads\n",
        "\n",
        "        self.key = nn.Linear(d_model, d_model)    # (d_model, n_heads * head_size)\n",
        "        self.query = nn.Linear(d_model, d_model)  # (d_model, n_heads * head_size)\n",
        "        self.value = nn.Linear(d_model, d_model)  # (d_model, n_heads * head_size)\n",
        "        self.proj = nn.Linear(d_model, d_model)   # (d_model, n_heads * head_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # mask.shape = (batch_size, 1, 1, max_len)\n",
        "\n",
        "        B, T, D = x.shape  # B - batch size, T - max_len, D - n_embd\n",
        "\n",
        "        k = self.key(x).view(B, -1, self.n_heads, self.head_size).permute(0, 2, 1, 3)    # (B, n_heads, T, head_size)\n",
        "        q = self.query(x).view(B, -1, self.n_heads, self.head_size).permute(0, 2, 1, 3)  # (B, n_heads, T, head_size)\n",
        "        v = self.value(x).view(B, -1, self.n_heads, self.head_size).permute(0, 2, 1, 3)  # (B, n_heads, T, head_size)\n",
        "\n",
        "        # (B, n_heads, T, head_size) @ (B, n_heads, head_size, T) -> (B, n_heads, T, T)\n",
        "        att_scores = (q @ k.permute(0, 1, 3, 2)) * (1.0 / math.sqrt(self.head_size))\n",
        "\n",
        "        att_scores = att_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        att_scores = nn.functional.softmax(att_scores, -1)\n",
        "        att_scores = self.dropout(att_scores)  # suppose adding dropout layer here is the original way\n",
        "\n",
        "        # (B, n_heads, T, T) * (B, n_heads, T, head_size) -> (B, n_heads, T, head_size)\n",
        "        out = att_scores @ v\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T,\n",
        "                                                    self.n_heads * self.head_size)  # (B, T, n_heads * head_size)\n",
        "\n",
        "        return self.proj(out)  # (B, T, n_heads * head_size) - original shape\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, 4 * d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * d_model, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ffwd = FeedForward(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x: (batch_size, max_len, n_embd)\n",
        "        # encoder mask: (batch_size, 1, max_len, max_len)\n",
        "        # result: (batch_size, max_len, d_model)\n",
        "        x = x + self.dropout(self.sa(self.ln1(x), mask))\n",
        "        x = x + self.dropout(self.ffwd(self.ln2(x)))\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class MaskedLanguageModel(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(d_model, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.softmax(self.lin(x))  # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "class NextSentencePrediction(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(d_model, 2)\n",
        "        self.softmax = nn.LogSoftmax(-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # use only the first token which is the [CLS]\n",
        "        # (B, T, D) -> (B, 2)\n",
        "        return self.softmax(self.lin(x[:,0]))  # x[:,0] <=> x[:,0, :]\n",
        "\n",
        "\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self, n_layers, vocab_size, d_model, n_heads, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embed = BERTEmbeddings(d_model, vocab_size, max_len, dropout)\n",
        "        self.blocks = nn.ModuleList([Block(d_model, n_heads, dropout) for _ in range(n_layers)])\n",
        "        self.mlm = MaskedLanguageModel(d_model, vocab_size)\n",
        "        self.nsp = NextSentencePrediction(d_model)\n",
        "\n",
        "    def forward(self, ids, segment_label):\n",
        "        # shape of ids is (batch_size, seq_len)\n",
        "        # attention masking for padded token\n",
        "        # mask = (ids > 0).unsqueeze(1).repeat(1, ids.size(1), 1).unsqueeze(1)  # (batch_size, seq_len) -> (batch_size, 1, seq_len, seq_len)\n",
        "        mask = (ids != 0).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "        x = self.embed(ids, segment_label)\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        # (B,T,vocab_size), (B,2)\n",
        "        return self.mlm(x), self.nsp(x)\n",
        "\n",
        "\n",
        "class ScheduledOptim:\n",
        "    \"\"\"Wrapper for optimizer with warmup scheduling and checkpoint support.\"\"\"\n",
        "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self._step = 0  # current step count\n",
        "        self.init_lr = np.power(d_model, -0.5)\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"\"\"Increment step, update LR, then step optimizer.\"\"\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        return min(\n",
        "            self._step ** -0.5,\n",
        "            self.n_warmup_steps ** -1.5 * self._step\n",
        "        )\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        \"\"\"Update LR on each step (warmup + decay).\"\"\"\n",
        "        self._step += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "    def state_dict(self):\n",
        "        \"\"\"Return scheduler state for checkpointing.\"\"\"\n",
        "        return {\n",
        "            '_step': self._step,\n",
        "            'warmup_steps': self.n_warmup_steps,\n",
        "            'optimizer': self._optimizer.state_dict()\n",
        "        }\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        \"\"\"Load scheduler state (step count, warmup, and optimizer state).\"\"\"\n",
        "        self._step = state_dict['_step']\n",
        "        self.n_warmup_steps = state_dict['warmup_steps']\n",
        "        self._optimizer.load_state_dict(state_dict['optimizer'])\n",
        "\n",
        "\n",
        "class BertTrainer:\n",
        "    def __init__(\n",
        "            self,\n",
        "            model,\n",
        "            train_dataloader,\n",
        "            test_dataloader=None,\n",
        "            lr=1e-4,\n",
        "            weight_decay=0.01,\n",
        "            betas=(0.9, 0.999),\n",
        "            warmup_steps=10000,\n",
        "            log_freq=10,\n",
        "            device='cuda'\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.test_dataloader = test_dataloader\n",
        "        self.log_freq = log_freq\n",
        "        self.device = device\n",
        "\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "        self.optim_schedule = ScheduledOptim(self.optim, self.model.d_model, warmup_steps)\n",
        "\n",
        "        self.mlm_criterion = nn.NLLLoss(ignore_index=0)  # ignore_index=0 tells the program to ignore [PAD] tokens\n",
        "        self.nsp_criterion = nn.NLLLoss()\n",
        "\n",
        "    def train(self, epoch):\n",
        "        self.iteration(epoch, self.train_dataloader)\n",
        "\n",
        "    def test(self, epoch):\n",
        "        self.iteration(epoch, self.test_dataloader, train=False)\n",
        "\n",
        "    def iteration(self, epoch, data_loader, train=True):\n",
        "\n",
        "        # Reset statistics for the current epoch\n",
        "        total_mlm_loss = 0\n",
        "        total_nsp_loss = 0\n",
        "        total_loss = 0\n",
        "\n",
        "        # For NSP accuracy\n",
        "        total_nsp_correct = 0\n",
        "        total_nsp_elements = 0 # Number of samples for NSP prediction\n",
        "\n",
        "        # For MLM accuracy (more complex, requires ignoring padded tokens and non-masked tokens)\n",
        "        total_mlm_correct = 0\n",
        "        total_mlm_elements = 0 # Number of masked tokens\n",
        "\n",
        "        mode = 'train' if train else 'test'\n",
        "\n",
        "        # Set model to train/eval mode\n",
        "        if train:\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval() # Use eval mode for test/validation to disable dropout/batchnorm\n",
        "\n",
        "\n",
        "        # progress bar\n",
        "        # Add `leave=True` if you want the progress bar to remain on screen after completion\n",
        "        data_iter = tqdm.tqdm(\n",
        "            enumerate(data_loader),\n",
        "            desc=\"EP_%s:%d\" % (mode, epoch),\n",
        "            total=len(data_loader),\n",
        "            bar_format=\"{l_bar}{r_bar}\",\n",
        "            leave=True\n",
        "        )\n",
        "\n",
        "        for i, batch in data_iter:\n",
        "            batch = {key: value.to(self.device) for key, value in batch.items()}\n",
        "\n",
        "            # Set gradients to zero only if training\n",
        "            if train:\n",
        "                self.optim_schedule.zero_grad()\n",
        "\n",
        "            mlm_output, nsp_output = self.model(batch[\"input_ids\"], batch[\"segment_label\"])\n",
        "\n",
        "            # Calculate MLM Loss\n",
        "            # mlm_output: (B, T, V), target: (B, T)\n",
        "            # NLLLoss expects (B, V, T) for input\n",
        "            mlm_loss = self.mlm_criterion(mlm_output.transpose(1, 2), batch[\"input_label\"])\n",
        "\n",
        "            # Calculate NSP Loss\n",
        "            # nsp_output: (B, 2), target: (B,)\n",
        "            nsp_loss = self.nsp_criterion(nsp_output, batch[\"is_next\"])\n",
        "\n",
        "            loss = mlm_loss + nsp_loss\n",
        "\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                self.optim_schedule.step_and_update_lr()\n",
        "\n",
        "            # Update total losses\n",
        "            total_mlm_loss += mlm_loss.item()\n",
        "            total_nsp_loss += nsp_loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate NSP Accuracy\n",
        "            predicted_next_sentence_labels = nsp_output.argmax(dim=-1) # Use dim=-1 for argmax\n",
        "            current_nsp_correct = (predicted_next_sentence_labels == batch[\"is_next\"]).sum().item()\n",
        "            current_nsp_elements = batch[\"is_next\"].nelement() # Total NSP samples in current batch\n",
        "\n",
        "            total_nsp_correct += current_nsp_correct\n",
        "            total_nsp_elements += current_nsp_elements\n",
        "\n",
        "            # Calculate MLM Accuracy\n",
        "            predicted_mlm_labels = mlm_output.argmax(dim=-1) # (B, T)\n",
        "            # Find which labels are actually masked (i.e., not 0 / PAD)\n",
        "            masked_positions = (batch[\"input_label\"] != 0) # Boolean mask for masked tokens\n",
        "            current_mlm_correct = (predicted_mlm_labels[masked_positions] == batch[\"input_label\"][masked_positions]).sum().item()\n",
        "            current_mlm_elements = masked_positions.sum().item() # Count of actual masked tokens\n",
        "\n",
        "            total_mlm_correct += current_mlm_correct\n",
        "            total_mlm_elements += current_mlm_elements\n",
        "\n",
        "\n",
        "            # Calculate metrics for the *current* iteration for `post_fix`\n",
        "            current_avg_loss = total_loss / (i + 1)\n",
        "            current_nsp_acc = (total_nsp_correct / total_nsp_elements * 100) if total_nsp_elements > 0 else 0\n",
        "            current_mlm_acc = (total_mlm_correct / total_mlm_elements * 100) if total_mlm_elements > 0 else 0\n",
        "\n",
        "\n",
        "            post_fix = {\n",
        "                \"epoch\": epoch,\n",
        "                \"iter\": i,\n",
        "                \"loss\": loss.item(), # Current batch loss\n",
        "                \"avg_loss\": current_avg_loss, # Average loss for epoch so far\n",
        "                \"mlm_loss\": mlm_loss.item(),\n",
        "                \"nsp_loss\": nsp_loss.item(),\n",
        "                \"nsp_acc\": current_nsp_acc, # Average NSP accuracy for epoch so far\n",
        "                \"mlm_acc\": current_mlm_acc # Average MLM accuracy for epoch so far\n",
        "            }\n",
        "\n",
        "            # Update the progress bar's postfix\n",
        "            data_iter.set_postfix(post_fix)\n",
        "\n",
        "            # Old way of writing to console, `set_postfix` is more integrated with tqdm\n",
        "            # if i % self.log_freq == 0:\n",
        "            #     data_iter.write(str(post_fix))\n",
        "\n",
        "            # SAVE CHECKPOINTS EVERY 30 STEPS\n",
        "            if train and (i + 1) % 30 == 0:\n",
        "                checkpoint = {\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': self.optim_schedule._optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': self.optim_schedule.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'step': self.optim_schedule._step\n",
        "                }\n",
        "                checkpoint_path = '/content/drive/MyDrive/bert-checkpoint/bert_checkpoint.pt'\n",
        "                torch.save(checkpoint, checkpoint_path)\n",
        "                print(f\"\\nSaved checkpoint at epoch {epoch}, step {i+1}\")\n",
        "\n",
        "        # print information after each epoch\n",
        "        # Calculate final epoch metrics\n",
        "        final_avg_loss = total_loss / len(data_iter)\n",
        "        final_nsp_acc = (total_nsp_correct / total_nsp_elements * 100) if total_nsp_elements > 0 else 0\n",
        "        final_mlm_acc = (total_mlm_correct / total_mlm_elements * 100) if total_mlm_elements > 0 else 0\n",
        "\n",
        "        print(\n",
        "            f\"EP{epoch}, {mode}: \\\n",
        "            avg_loss={final_avg_loss:.4f}, \\\n",
        "            nsp_acc={final_nsp_acc:.2f}%, \\\n",
        "            mlm_acc={final_mlm_acc:.2f}%\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "CSd1qOiCF0xJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_path = '/content/drive/Othercomputers/My_Laptop/bert-it-1/bert-it-vocab.txt'\n",
        "tokenizer = BertTokenizer.from_pretrained(vocab_path, local_files_only=True)\n",
        "\n",
        "print(\"Tokenizer loaded successfully!\")\n",
        "print(f\"Vocab size: {len(tokenizer)}\")\n",
        "\n",
        "print(tokenizer(\"ovo je reƒçenica\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-BqP-xbdc-e",
        "outputId": "b18261bb-9117-4ada-b1a0-2713ed348515"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1883: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded successfully!\n",
            "Vocab size: 30000\n",
            "{'input_ids': [1, 2054, 1865, 16573, 1008, 1941, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = BertDataset_v1(train_dir, tokenizer, seq_len=max_len)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "print(next(iter(train_dataloader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHQKNk5LO-NT",
        "outputId": "deb1fb49-7386-4632-e8d0-9101e9b63a17"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Dataset Init] Counting lines across all batch files...\n",
            "[Dataset Init] Found 33620000 total samples across 3362 files.\n",
            "{'input_ids': tensor([[    1,  8101,     3,  ...,     0,     0,     0],\n",
            "        [    1,  2341, 12720,  ...,     0,     0,     0],\n",
            "        [    1, 21053,  5892,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [    1,  2054, 28568,  ...,     0,     0,     0],\n",
            "        [    1,     3, 22566,  ...,     0,     0,     0],\n",
            "        [    1,  2573,    16,  ...,     0,     0,     0]]), 'input_label': tensor([[    0,     0,  6702,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,     0,     0,     0],\n",
            "        [    0, 26442,     0,  ...,     0,     0,     0],\n",
            "        [    0,     0,     0,  ...,     0,     0,     0]]), 'segment_label': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'is_next': tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
            "        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
            "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = BERT(\n",
        "    n_layers,\n",
        "    len(tokenizer.vocab),\n",
        "    n_embd,\n",
        "    n_heads,\n",
        "    max_len,\n",
        "    dropout\n",
        ")\n",
        "\n",
        "bert_trainer = BertTrainer(bert_model, train_dataloader, device=device)\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    bert_trainer.train(epoch)"
      ],
      "metadata": {
        "id": "ROxaDEiaGJf_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "6ba2b77e-1674-4982-e0e5-9e36586be22a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 30/525313 [00:43<186:15:12,  1.28s/it, epoch=0, iter=29, loss=11.9, avg_loss=12, mlm_loss=11.1, nsp_loss=0.782, nsp_acc=49.9, mlm_acc=0]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 60/525313 [01:23<197:27:46,  1.35s/it, epoch=0, iter=59, loss=11.9, avg_loss=11.9, mlm_loss=11.1, nsp_loss=0.769, nsp_acc=50.8, mlm_acc=0.00329]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 90/525313 [02:03<181:15:47,  1.24s/it, epoch=0, iter=89, loss=11.8, avg_loss=11.9, mlm_loss=11, nsp_loss=0.761, nsp_acc=50.5, mlm_acc=0.00221]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 120/525313 [02:43<186:48:43,  1.28s/it, epoch=0, iter=119, loss=11.8, avg_loss=11.9, mlm_loss=11, nsp_loss=0.774, nsp_acc=50.6, mlm_acc=0.00166]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 150/525313 [03:26<192:32:20,  1.32s/it, epoch=0, iter=149, loss=11.7, avg_loss=11.9, mlm_loss=11, nsp_loss=0.71, nsp_acc=50.1, mlm_acc=0.00266]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 180/525313 [04:06<184:27:01,  1.26s/it, epoch=0, iter=179, loss=11.6, avg_loss=11.8, mlm_loss=10.9, nsp_loss=0.662, nsp_acc=50.1, mlm_acc=0.00332]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 210/525313 [04:46<185:37:32,  1.27s/it, epoch=0, iter=209, loss=11.6, avg_loss=11.8, mlm_loss=10.8, nsp_loss=0.811, nsp_acc=50, mlm_acc=0.00284]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 240/525313 [05:27<187:39:43,  1.29s/it, epoch=0, iter=239, loss=11.5, avg_loss=11.8, mlm_loss=10.7, nsp_loss=0.753, nsp_acc=50, mlm_acc=0.00249]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 270/525313 [06:06<180:00:49,  1.23s/it, epoch=0, iter=269, loss=11.5, avg_loss=11.7, mlm_loss=10.7, nsp_loss=0.817, nsp_acc=49.9, mlm_acc=0.00221]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 300/525313 [06:46<183:59:16,  1.26s/it, epoch=0, iter=299, loss=11.4, avg_loss=11.7, mlm_loss=10.6, nsp_loss=0.81, nsp_acc=49.8, mlm_acc=0.00199]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 330/525313 [07:25<204:59:54,  1.41s/it, epoch=0, iter=329, loss=11.2, avg_loss=11.7, mlm_loss=10.5, nsp_loss=0.691, nsp_acc=49.9, mlm_acc=0.00785]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 360/525313 [08:06<188:53:57,  1.30s/it, epoch=0, iter=359, loss=11.2, avg_loss=11.6, mlm_loss=10.4, nsp_loss=0.78, nsp_acc=49.7, mlm_acc=0.0188]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 390/525313 [08:46<185:48:21,  1.27s/it, epoch=0, iter=389, loss=11.1, avg_loss=11.6, mlm_loss=10.3, nsp_loss=0.736, nsp_acc=49.6, mlm_acc=0.0509]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 420/525313 [09:25<184:12:37,  1.26s/it, epoch=0, iter=419, loss=11.1, avg_loss=11.6, mlm_loss=10.4, nsp_loss=0.783, nsp_acc=49.6, mlm_acc=0.114]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 450/525313 [10:05<183:04:05,  1.26s/it, epoch=0, iter=449, loss=11.1, avg_loss=11.5, mlm_loss=10.3, nsp_loss=0.732, nsp_acc=49.7, mlm_acc=0.222]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 480/525313 [10:44<186:23:34,  1.28s/it, epoch=0, iter=479, loss=11, avg_loss=11.5, mlm_loss=10.2, nsp_loss=0.766, nsp_acc=49.6, mlm_acc=0.375]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint at epoch 0, step 480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EP_train:0:   0%|| 509/525313 [11:22<195:30:33,  1.34s/it, epoch=0, iter=508, loss=10.8, avg_loss=11.4, mlm_loss=10.1, nsp_loss=0.722, nsp_acc=49.7, mlm_acc=0.523]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-668726034.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mbert_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-11-2521498367.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-11-2521498367.py\u001b[0m in \u001b[0;36miteration\u001b[0;34m(self, epoch, data_loader, train)\u001b[0m\n\u001b[1;32m    259\u001b[0m         )\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-2990206507.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mt1_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_next_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Tokenize and get input IDs for t1 and t2 (after stripping)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-2990206507.py\u001b[0m in \u001b[0;36mget_sent\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mline_idx_in_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_line_offsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_file_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Get t1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mt1_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline_idx_in_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-2990206507.py\u001b[0m in \u001b[0;36mget_file_lines\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_file_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint_path = '/content/drive/MyDrive/bert-checkpoints/bert_checkpoint.pt'\n",
        "\n",
        "# checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# bert_model = BERT(\n",
        "#     n_layers,\n",
        "#     len(tokenizer.vocab),\n",
        "#     n_embd,\n",
        "#     n_heads,\n",
        "#     max_len,\n",
        "#     dropout\n",
        "# )\n",
        "# bert_trainer = BertTrainer(bert_model, train_dataloader, device=device)\n",
        "\n",
        "\n",
        "# bert_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# bert_trainer.optim_schedule.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "# epoch = checkpoint['epoch']\n",
        "\n",
        "# print('Checkpoint loaded successfully!')\n",
        "# print('Training continued...')\n",
        "\n",
        "# for ep in range(epoch, epochs):\n",
        "#     bert_trainer.train(ep)"
      ],
      "metadata": {
        "id": "xkU9z8Qbgcd2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOu2hNyO+wyq1uNWWnqSWC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}